<html>
    <head>
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
        <style>
            h1 {
                text-align: center;
                margin-bottom: 30px;
                color: #333;
            }

            h2 {
                color: #2c3e50;
                border-bottom: 2px solid #3498db;
                padding-bottom: 5px;
                margin-top: 40px;
                margin-bottom: 20px;
            }

            h3 {
                color: #34495e;
                margin-top: 25px;
                margin-bottom: 15px;
            }

            h4 {
                color: #7f8c8d;
                margin-top: 20px;
                margin-bottom: 10px;
            }

            .container {
                margin: 0 auto;
                padding: 60px 20%;
            }

            figure {
                text-align: center;
            }

            img {
                display: inline-block;
            }

            body {
                font-family: 'Inter', sans-serif;
            }

            table {
                margin: 20px auto;
                border-collapse: collapse;
                width: 100%;
            }

            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }

            th {
                background-color: #f2f2f2;
                font-weight: 600;
            }

            code {
                background-color: #f5f5f5;
                padding: 2px 4px;
                border-radius: 3px;
                font-family: 'Courier New', monospace;
            }

            pre {
                background-color: #f5f5f5;
                padding: 10px;
                border-radius: 5px;
                overflow-x: auto;
            }

            figcaption {
                font-style: italic;
                color: #666;
                margin-top: 8px;
            }

            a {
                color: #3498db;
                text-decoration: none;
            }

            a:hover {
                text-decoration: underline;
            }

            .header-links {
                text-align: center;
                margin: 20px 0;
                padding: 15px;
                background-color: #f8f9fa;
                border-radius: 5px;
            }

            ul, ol {
                line-height: 1.6;
            }

            p {
                line-height: 1.6;
                margin-bottom: 15px;
            }
        </style>
    </head>
    <body>
        <div class="container">
        <h1>CS184/284A Spring 2025 Homework 1 Write-Up</h1>
        
        <div class="header-links">
            <div style="margin-bottom: 10px;"><strong>Name:</strong> Anisha Iyer</div>
            <div style="margin-bottom: 10px;"><strong>Webpage:</strong> <a href="https://anishaiyer27.github.io/hw-webpages-anishaiyer/hw1/index.html">https://anishaiyer27.github.io/hw-webpages-anishaiyer/hw1/index.html</a></div>
            <div><strong>GitHub Repository:</strong> <a href="https://github.com/cal-cs184/hw-rasterizer-aiyer.git">https://github.com/cal-cs184/hw-rasterizer-aiyer.git</a></div>
        </div>

        <h2>Overview</h2>
        In Homework 1, I have implemented a complete 2D graphics rasterizer that demonstrates fundamental computer graphics principles from the ground up. The system
        includes triangle rasterization with point-in-triangle testing using edge functions, supersampling anti-aliasing to reduce jagged edges, and
        optimized performance variants with incremental algorithms. I built a hierarchical transformation system supporting complex SVG scenes with nested groups,
        rotations, and scaling, exemplified by an expressive robot character that showcases dynamic poses through matrix composition. The advanced texture mapping
        implementation features both nearest-neighbor and bilinear interpolation for pixel sampling, along with a complete mipmap system supporting three level sampling
        modes: basic texture sampling (L_ZERO), automatic mipmap level selection (L_NEAREST), and trilinear filtering (L_LINEAR) for smooth transitions. This project provided
        me insights into the translation of theoretical graphics concepts into practical visual systems, starting from sampling theory and aliasing reduction to hierarchical transformations
        and level-of-detail management to build a system that mirrors the structure of modern GPU rendering pipelines.

        <h2>Task 1: Drawing Single-Color Triangles</h2>
        
        <h3>Algorithm Implementation</h3>
        <p>The triangle rasterization algorithm that I implemented follows a systematic approach to efficiently render triangles by testing pixel coverage within a bounded region.</p>

        <h3>Bounding Box Calculation</h3>
        <p>The algorithm begins by determining the smallest axis-aligned rectangle that contains the entire triangle. 
            This involves computing the minimum and maximum x and y coordinates across the three vertices. 
            Restricting rasterization to this region avoids unnecessary computation outside the triangle's extent.</p>

        <h3>Edge Function Setup</h3>
        <p>For each edge of the triangle, an edge function is defined using the expression:</p>
        <pre><code>E(x,y) = (x - x0)(y1 - y0) - (y - y0)(x1 - x0)</code></pre>
        <p>Where <code>(x0,y0)</code> and <code>(x1,y1)</code> are the edge endpoints.</p>
        <p>This function determines whether a point lies on the "inside" or "outside" of an edge, 
            which is important for determining pixel coverage. The edge function returns a positive value 
            for points on one side of the edge and a negative value for points on the other side.</p>

        <h3>Pixel Iteration and Sampling</h3>
        <p>The algorithm iterates over every pixel within the bounding box. Instead of testing the pixel's top-left corner, 
            sampling occurs at the pixel center using coordinates <code>(x + 0.5, y + 0.5)</code>, 
            as required by the specification. This approach improves accuracy and ensures proper coverage testing.</p>

        <h3>Point-in-Triangle Test</h3>
        <p>At each sample point, all three edge functions are evaluated. If all three return values 
            with the same sign (either all positive or all negative), the point is inside the triangle.  
            This test works for both clockwise and counter-clockwise winding orders without requiring additional logic.</p>

        <h3>Supersampling Extension</h3>
        <p>The rasterizer was extended to support multiple samples per pixel for antialiasing. 
            Multiple sample values are calculated at fractional offsets within each pixel 
            and then averaged in <code>resolve_to_framebuffer()</code> to produce smooth antialiased edges.</p>

        <h3>Algorithm Efficiency</h3>
        <p>This algorithm is designed to be as efficient as possible without skipping necessary evaluations. In terms of <strong>spatial scope</strong>, the algorithm never iterates outside the triangle's bounding box. For <strong>time complexity</strong>, each pixel is tested in constant time, so total cost scales with bounding box area. The algorithm's O(bbox_area) complexity ensures efficient rendering, as each pixel is tested exactly once. Most importantly, there is <strong>no wasted computation</strong> - every pixel tested might contribute to the final render.</p>

        <h3>Extra Credit: Performance Optimizations</h3>
        <p>Beyond the basic bounding box optimization, I implemented several performance improvements in the supersampling rasterization pipeline. Instead of computing edge functions from scratch for each sample point, I implemented incremental evaluation using the fact that edge functions are linear. For horizontal traversal, the edge function value changes by a constant <code>(y1 - y0)</code>, and for vertical traversal by <code>-(x1 - x0)</code>. This eliminates redundant multiplications and significantly reduces computational overhead.</p>

        <p>I also streamlined the sample buffer access pattern to minimize address calculations. By precomputing base indices and using incremental offsets, the implementation reduces the number of arithmetic operations needed for each sample location in the expanded buffer. Additionally, for pixels that are clearly outside the triangle (all edge functions have the same negative sign), the algorithm can skip processing remaining sub-pixel samples within that pixel, providing performance gains especially for larger triangles with high supersampling rates.</p>

        <p>These incremental algorithm optimizations demonstrate how mathematical properties of geometric functions can be leveraged to achieve significant performance improvements while maintaining identical visual output to the basic implementation.</p>

        <h3>Task 1 Screenshot</h3>
        <div style="display: flex; flex-direction: column; align-items: center;">
            <img src="TASK_1_fixed.png" width="600px"/>
            <figcaption>Screenshot of basic/test4.svg with default viewing parameters and pixel inspector centered on triangle edge showing aliasing artifacts</figcaption>
        </div>

        <h2>Task 2: Antialiasing by Supersampling</h2>

        <h3>Supersampling Comparison Screenshots</h3>
        <div style="display: flex; flex-direction: column; align-items: center;">
            <table style="width: 100%; text-align: center; border-collapse: collapse;">
              <tr>
                <td style="text-align: center;">
                  <img src="screenshot_7-11_1-58-28.png" width="400px"/>
                  <figcaption><strong>Sample Rate 1:</strong> Jagged edges with harsh transitions</figcaption>
                </td>
                <td style="text-align: center;">
                  <img src="screenshot_7-11_1-58-32.png" width="400px"/>
                  <figcaption><strong>Sample Rate 4:</strong> Smoother edges with intermediate grayscale</figcaption>
                </td>
              </tr>
              <tr>
                <td style="text-align: center;">
                  <img src="screenshot_7-11_1-58-33.png" width="400px"/>
                  <figcaption><strong>Sample Rate 9:</strong> Even smoother antialiasing</figcaption>
                </td>
                <td style="text-align: center;">
                  <img src="s4.png" width="400px"/>
                  <figcaption><strong>Sample Rate 16:</strong> Very smooth edges with clean gradients</figcaption>
                </td>
              </tr>
            </table>
        </div>

        <h3>Algorithm Overview</h3>
        <p>The supersampling algorithm uses <strong>high-resolution sampling</strong> where for each pixel, <code>√sample_rate × √sample_rate</code> samples are taken, arranged in a uniform grid pattern within the pixel. For <strong>sample buffer storage</strong>, each sub-pixel sample is stored in a high-resolution <code>sample_buffer</code> that is <code>sample_rate</code> times larger than the final framebuffer. Finally, during <strong>downsampling</strong>, after rendering, <code>resolve_to_framebuffer()</code> averages all sub-pixel samples to compute the final pixel color.</p>

        <h3>Data Structure Modifications</h3>
        <p>The implementation required several key modifications to support supersampling:</p>
        
        <h4>Sample Buffer Expansion</h4>
        <p>The <code>sample_buffer</code> was expanded from <code>width × height</code> to <code>width × height × sample_rate</code> to store all sub-pixel samples. This allows independent storage of each sample within a pixel before final averaging.</p>

        <h4>Rasterization Pipeline Changes</h4>
        <p>Triangle rasterization now iterates through all sub-pixel samples within each pixel. For each sub-pixel location <code>(x + dx, y + dy)</code>, where <code>dx</code> and <code>dy</code> are fractional offsets, the point-in-triangle test is performed and results stored in the expanded sample buffer.</p>

        <h4>Resolution and Averaging</h4>
        <p>The <code>resolve_to_framebuffer()</code> function averages all samples within each pixel to compute the final color. This averaging process naturally produces intermediate gray levels that smooth out jagged edges, creating the antialiasing effect.</p>

        <h3>Why Supersampling Works</h3>
        <p>Supersampling is effective because it addresses the fundamental cause of aliasing: insufficient sampling frequency. By taking multiple samples per pixel and averaging them, we effectively increase the sampling rate and reduce high-frequency artifacts. The averaging process acts as a low-pass filter, smoothing out sharp transitions and creating more visually pleasing results.</p>

        <h3>Results Analysis</h3>
        <p>The comparison screenshots demonstrate the progressive improvement in edge quality as sample rate increases. At sample rate 1, triangle edges appear jagged with harsh pixel-level transitions. Sample rate 4 introduces intermediate gray values along edges, significantly reducing jaggedness. Sample rate 16 produces very smooth edges with clean color gradients. These results occur because supersampling captures partial pixel coverage at triangle edges, creating intermediate gray values that smooth the visual transition from triangle interior to background. Each increase in sample rate provides diminishing returns, but the visual improvement is clear, especially along diagonal edges and thin geometric features.</p>

        <h2>Task 3: Transforms</h2>
        
        <h3>Robot Design Implementation</h3>
        <p>The Cubeman implementation demonstrates hierarchical transformations through a robot character in a dynamic, colorful pose. The robot is tilted 15 degrees from vertical and performs an energetic waving gesture with asymmetric arm positioning. One arm is raised upward in a greeting motion while the other extends outward, and the robot's head is dramatically tilted, creating a playful and expressive character that effectively demonstrates complex hierarchical transformations.</p>

        <p>My robot uses a vibrant multi-colored scheme to create visual interest and distinguish body parts. The <strong>head</strong> is rendered in bright green, the <strong>torso</strong> in blue, the <strong>left arm</strong> in pink gradient tones, and the <strong>right arm</strong> in cyan shades. The <strong>left leg</strong> uses yellow-orange colors while the <strong>right leg</strong> employs purple hues, creating a rainbow robot aesthetic.</p>

        <h3>Hierarchical Transformation Implementation</h3>
        <p>The dynamic pose demonstrates complex hierarchical transformations through multiple nested rotation and scaling operations. The entire robot is rotated <code>rotate(15)</code> for the tilted stance. The head combines <code>translate(0 -100)</code>, <code>rotate(75)</code>, and <code>scale(.6 .6)</code> for dramatic positioning. The left arm uses <code>translate(-90 -60) rotate(-30)</code> for the raised greeting pose, while the right arm employs <code>translate(90 -20) rotate(45)</code> with an additional forearm rotation <code>rotate(30)</code> for the waving gesture. The right leg includes <code>rotate(-10)</code> for asymmetric stance. This multi-layered approach illustrates how expressive character poses emerge from the thoughtful composition of geometric transformations.</p>

        <div style="display: flex; flex-direction: column; align-items: center;">
            <img src="robot_final.png" width="400px"/>
            <figcaption>Colorful robot character demonstrating complex hierarchical transformations in a dynamic waving pose</figcaption>
        </div>

        <h2>Task 4: Barycentric coordinates</h2>
        
        <h3>Algorithm Implementation</h3>
        <p>The barycentric coordinate implementation extends the triangle rasterization approach from Task 1 to enable smooth color interpolation across triangle surfaces. The algorithm reuses the core triangle rasterization logic while adding coordinate calculation and color blending capabilities.</p>

        <p>For each sample point within a triangle, barycentric coordinates are calculated using the area method. These coordinates are then used to interpolate vertex colors using the formula <code>α * c0 + β * c1 + γ * c2</code>. The implementation supports both single sampling and supersampling modes, maintaining compatibility with the antialiasing framework from Task 2. Degenerate triangles are handled through area validation to prevent numerical instabilities.</p>

        <h3>Barycentric Coordinates Theory</h3>
        <p>Barycentric coordinates (<code>α, β, γ</code>) provide a method for representing any point <code>P</code> inside a triangle as a weighted combination of the three vertices:</p>
        <p><code>P = α * A + β * B + γ * C</code></p>
        <p>Where <code>α + β + γ = 1</code></p>
        
        <p>The coordinate system has several important properties. In terms of <strong>vertex influence</strong>, each coordinate represents the "influence" of that vertex on the point. For <strong>normalized range</strong>, values range from 0 to 1, where 1 means the point coincides with that vertex. The <strong>distance relationship</strong> means that points closer to a vertex have higher coordinate values for that vertex. Finally, for <strong>linear interpolation</strong>, any attribute can be smoothly interpolated using the same weights.</p>
        
        <h3>Visual Demonstration</h3>
        <p>To illustrate barycentric coordinates visually, we consider a triangle with three differently colored vertices. Each point inside the triangle displays a color that is the weighted average of the three vertex colors- the weights being the barycentric coordinates themselves.</p>

        <div style="display: flex; flex-direction: column; align-items: center;">
            <img src="screenshot_7-11_2-37-34.png" width="400px"/>
            <figcaption>Triangle with red, green, and blue vertices showing smooth color interpolation using barycentric coordinates</figcaption>
        </div>

        <h3>Color Interpolation Process</h3>
        <p>For color interpolation, the barycentric coordinates serve as blending weights:</p>
        <p><code>Color(P) = α * Color(A) + β * Color(B) + γ * Color(C)</code></p>
        <p>This approach creates smooth color gradients across the triangle surface, with colors blending naturally from vertex to vertex. The resulting interpolation maintains visual continuity while providing precise control over color distribution within each triangle.</p>

        <div style="display: flex; flex-direction: column; align-items: center;">
            <img src="TASK_4.png" width="600px"/>
            <figcaption>Screenshot of svg/basic/test7.svg showing barycentric coordinate color interpolation</figcaption>
        </div>

        <h2>Task 5: "Pixel sampling" for texture mapping</h2>
        <h3>Implementation Summary</h3>

        <h4>Textured Triangle Rasterization</h4>
        <p>The textured triangle rasterization approach reuses the barycentric coordinate approach from Task 4, interpolates texture coordinates (u, v) using barycentric weights, constructs a <code>SampleParams</code> struct and calls <code>tex.sample()</code> at each pixel, and supports both single sampling and supersampling modes.</p>

        <h4>Nearest Neighbor Sampling</h4>
        <p>Nearest neighbor sampling converts normalized UV coordinates to texel space, rounds to the nearest integer texel coordinates, and clamps values to valid texture bounds before sampling a single texel.</p>

        <h4>Bilinear Sampling</h4>
        <p>Bilinear sampling identifies the four surrounding texels in texture space, computes interpolation weights based on fractional UV components, and performs 2D linear interpolation across the four sampled texel colors.</p>

        <h3>Pixel Sampling Explanation</h3>
        <p>
        Pixel sampling refers to the method by which texture information is retrieved and used to shade fragments during rasterization. To determine the correct color at each pixel, we first compute the corresponding point in the texture using barycentric interpolation of the UV coordinates. We then apply one of two sampling methods (nearest or bilinear) to select the final color.
        </p>

        <p>
        The implementation uses a <code>SampleParams</code> struct that captures UV coordinates and sampling resolution. This structure is passed to the <code>tex.sample()</code> function, which handles both nearest and bilinear methods depending on the selected sampling mode. The same sampling logic is applied for both single-pixel sampling and supersampling cases.
        </p>

        <h3>Comparison of Sampling Methods</h3>
        <p>
        Below are four screenshots taken using the <code>'S'</code> hotkey from a texture-mapped SVG file where the difference between sampling methods is clearly visible.
        </p>

        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin: 1em 0;">
        <figure>
            <img src="TASK_5_1_ne.png" alt="Nearest Sampling, 1 sample per pixel" style="width: 100%;">
            <figcaption>Nearest Sampling, 1 Sample per Pixel</figcaption>
        </figure>
        <figure>
            <img src="TASK_5_16_ne.png" alt="Nearest Sampling, 16 samples per pixel" style="width: 100%;">
            <figcaption>Nearest Sampling, 16 Samples per Pixel</figcaption>
        </figure>
        <figure>
            <img src="TASK_5_1_bi.png" alt="Bilinear Sampling, 1 sample per pixel" style="width: 100%;">
            <figcaption>Bilinear Sampling, 1 Sample per Pixel</figcaption>
        </figure>
        <figure>
            <img src="TASK_5_16_bi.png" alt="Bilinear Sampling, 16 samples per pixel" style="width: 100%;">
            <figcaption>Bilinear Sampling, 16 Samples per Pixel</figcaption>
        </figure>
        </div>

        <h3>Discussion</h3>
        <p>
        Bilinear sampling produces smoother results by blending colors from adjacent texels, which significantly reduces aliasing, especially along diagonal or high-frequency texture regions. Nearest neighbor, on the other hand, selects a single texel and can result in blocky artifacts and visible jaggedness when texture resolution is low or under strong transformations.
        </p>

        <p>
        The difference between nearest and bilinear sampling is most pronounced when textures are magnified (zoomed in) or when viewing high-frequency texture details at oblique angles. In the screenshots above showing the Campanile and surrounding architecture, the contrast is particularly evident in several key areas: the fine architectural details of the Campanile facades show much smoother transitions with bilinear sampling, the diagonal edges of the tower structure appear less jagged, and the textural elements in the foliage areas demonstrate how bilinear interpolation creates more natural color gradations compared to the harsh pixel boundaries visible in nearest neighbor sampling.
        </p>

        <p>
        The comparison becomes especially clear when examining the granular visual details of the tower's gaps and stone textures, where nearest neighbor sampling creates visible blockiness that bilinear sampling effectively smooths out. Supersampling helps mitigate aliasing in both methods, but bilinear sampling generally outperforms nearest neighbor in producing visually smooth transitions and better overall image quality, particularly for complex architectural textures like those shown in the image of the Campanile.
        </p>

        <h2>Task 6: "Level sampling" with mipmaps for texture mapping</h2>
        
        <h3>Implementation Overview</h3>
        <p>The level sampling implementation provides mipmap support with three sampling modes. <strong>L_ZERO</strong> always uses full resolution (level 0), <strong>L_NEAREST</strong> calculates appropriate mipmap level and rounds to nearest integer, and <strong>L_LINEAR</strong> uses trilinear sampling to interpolate between two adjacent mipmap levels.</p>

        <h3>Level Calculation</h3>
        <p>Mipmap levels are calculated using texture coordinate derivatives to determine the rate of texture change across screen pixels. The implementation computes the UV coordinate gradients for neighboring pixels and uses the maximum gradient magnitude to select the appropriate detail level.</p>
        
        <p>Specifically, the algorithm calculates the derivatives <code>du/dx</code>, <code>du/dy</code>, <code>dv/dx</code>, and <code>dv/dy</code> using finite differences between adjacent screen pixels. The mipmap level is then determined using the formula <code>level = log2(max(||∇u||, ||∇v||))</code>, where the gradient magnitudes represent the rate of texture coordinate change. This approach prevents aliasing during texture minification by pre-filtering high frequencies that would otherwise cause temporal artifacts.</p>

        <h3>Sampling Method Comparison</h3>
        <p>The three techniques (pixel sampling, level sampling, and supersampling) offer different tradeoffs:</p>

        <table>
            <thead>
            <tr>
                <th>Technique</th>
                <th>Speed</th>
                <th>Memory Usage</th>
                <th>Antialiasing Power</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>Pixel Sampling (Nearest vs Bilinear)</td>
                <td>Nearest: Fast<br>Bilinear: Moderate</td>
                <td>Same</td>
                <td>Bilinear provides texture smoothing</td>
            </tr>
            <tr>
                <td>Level Sampling (L_ZERO, L_NEAREST, L_LINEAR)</td>
                <td>L_ZERO: Fast<br>L_NEAREST: Moderate<br>L_LINEAR: Slower</td>
                <td>Higher (mipmap storage)</td>
                <td>Reduces aliasing from minification</td>
            </tr>
            <tr>
                <td>Supersampling (1x, 4x, 16x)</td>
                <td>Decreases significantly with sample rate</td>
                <td>Scales linearly with sample rate</td>
                <td>Strongest antialiasing effect</td>
            </tr>
            </tbody>
        </table>

        <h3>Task 6 Screenshots</h3>
        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 16px; margin: 1em 0;">
        <figure>
            <img src="TASK_6_Ne_0.png" alt="L_ZERO and P_NEAREST" style="width: 100%;">
            <figcaption>L_ZERO and P_NEAREST</figcaption>
        </figure>
        <figure>
            <img src="TASK_6_Bi_0.png" alt="L_ZERO and P_LINEAR" style="width: 100%;">
            <figcaption>L_ZERO and P_LINEAR</figcaption>
        </figure>
        <figure>
            <img src="TASK_6_Ne_1.png" alt="L_NEAREST and P_NEAREST" style="width: 100%;">
            <figcaption>L_NEAREST and P_NEAREST</figcaption>
        </figure>
        <figure>
            <img src="TASK_6_Bi_1.png" alt="L_NEAREST and P_LINEAR" style="width: 100%;">
            <figcaption>L_NEAREST and P_LINEAR</figcaption>
        </figure>
        <figure>
            <img src="TASK_6_Ne_2.png" alt="L_LINEAR and P_NEAREST" style="width: 100%;">
            <figcaption>L_LINEAR and P_NEAREST</figcaption>
        </figure>
        <figure>
            <img src="TASK_6_Bi_2.png" alt="L_LINEAR and P_LINEAR" style="width: 100%;">
            <figcaption>L_LINEAR and P_LINEAR</figcaption>
        </figure>
        </div>

        <h2>Task 7: Extra Credit - Creative SVG Artwork</h2>

        <h3>Procedural Portrait Generation</h3>
        <p>This creative artwork demonstrates a procedural approach to SVG generation through a Python-based portrait creation system. Rather than manually crafting SVG elements, the implementation uses algorithmic generation to create variations of a stylized portrait with mathematical precision and controlled randomization in order to generate a display image representing Sabrina Carpenter.</p>
        
        <p>The artwork features a portrait composition with several algorithmically generated elements. Curly hair generation uses mathematical functions including sine waves and controlled randomness to create natural-looking zigzag patterns that simulate volumetric curls. Parametric facial structure employs polygon-based construction with precise coordinate calculations to maintain natural proportions. Systematic color palettes implement three distinct teal-based color schemes with coordinated relationships between hair, outfit, and background colors. Dynamic scene elements include procedurally placed musical notes with random positioning and rotation, plus teal stage lighting effects with varying opacity.</p>

        <div style="display: flex; flex-direction: column; align-items: center;">
            <img src="competition.png" width="600px"/>
            <figcaption>Procedurally generated portrait demonstrating algorithmic SVG creation with curly hair and teal color schemes</figcaption>
        </div>

        <h3>Technical Implementation</h3>
        <p>The implementation uses Python to generate SVG code through mathematical algorithms. Hair strand generation employs functions like <code>generate_curly_hair_strand()</code> that use modular arithmetic and sine functions to create realistic curl patterns with controlled randomness. Layered polygon construction builds complex shapes through coordinate transformation and point generation, with hair requiring up to 280-point polygons for smooth curves. Color palette management implements three distinct teal-based schemes using RGB color theory to ensure visual harmony. Procedural scene composition uses random number generation with bounded parameters to place musical notes, stage lights, and sparkle effects while maintaining compositional balance.</p>

        <h3>Algorithmic Features</h3>
        <p>The generation system demonstrates several computational graphics techniques. Curve generation uses mathematical functions including <code>math.sin(i * 0.3) * 10</code> to create natural hair flow patterns. Controlled randomization employs functions like <code>random.randint(-3, 3)</code> to add organic variation while maintaining structural integrity. Coordinate transformation implements systematic point generation for complex polygons, with zigzag patterns created through alternating directional logic. Variation generation produces multiple unique artworks through parameterized color schemes and randomized element placement, demonstrating how algorithmic approaches can create diverse artistic outputs from a single codebase.</p>

        <p>This implementation showcases how computational methods can enhance artistic creation, using mathematical precision to generate complex visual compositions while incorporating controlled randomness to achieve natural, organic aesthetics.</p>

        </div>
    </body>
</html>